{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elabo\\AppData\\Local\\Temp\\ipykernel_6008\\1957742264.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was too old on your system - pyarrow 10.0.1 is the current minimum supported version as of this release.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import torch \n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 100])\n",
      "torch.Size([1, 200])\n",
      "torch.Size([1, 200, 32])\n",
      "torch.Size([1, 200, 64])\n",
      "torch.Size([1, 200, 64]) torch.Size([1, 200, 64]) torch.Size([1, 200, 64])\n",
      "torch.Size([1, 200, 64])\n",
      "torch.Size([1, 12800])\n",
      "torch.Size([1, 6400])\n",
      "torch.Size([1, 3200])\n",
      "torch.Size([1, 1])\n",
      "tensor([[0.]], grad_fn=<ReluBackward0>)\n",
      "torch.Size([1, 1])\n",
      "tensor([[0.5000]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TunableAttentionRegression(torch.nn.Module):\n",
    "    def __init__(self, input_size = 2707, hidden_size = 64,\n",
    "                output_size = 1, numberOfHeads = 16) -> None:\n",
    "        super(TunableAttentionRegression, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_size, 32)\n",
    "        self.lstm = torch.nn.LSTM(32, hidden_size, batch_first=True)\n",
    "        self.attention = torch.nn.MultiheadAttention(hidden_size, num_heads=numberOfHeads, batch_first=True) #https://pytorch.org/docs/stable/generated/torch.ao.nn.quantizable.MultiheadAttention.html#multiheadattention\n",
    "        self.linear1 = torch.nn.Linear(hidden_size*200, hidden_size*100)\n",
    "        self.linear2 = torch.nn.Linear(hidden_size*100, hidden_size*50)\n",
    "        self.linear3 = torch.nn.Linear(hidden_size*50, output_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        print(x.shape)\n",
    "        x = x.view(x.size(0), x.size(2)*2)\n",
    "        print(x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "        print(embedded.shape)\n",
    "        # embedded = embedded.view(*x.size(), -1) #to make it 3D\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        print(lstm_out.shape)\n",
    "        # lstm_out = lstm_out.permute(1, 0, 2)  # [seq_len, batch, hidden_size]\n",
    "        # print(lstm_out.shape)\n",
    "        query = lstm_out.permute(0, 1, 2)\n",
    "        key = lstm_out.permute(0, 1, 2)\n",
    "        value = lstm_out.permute(0, 1, 2)\n",
    "        print(query.shape, key.shape, value.shape)\n",
    "        attention_output, _ = self.attention(query, key, value)\n",
    "        print(attention_output.shape)\n",
    "        attention_output_as_2d = attention_output.view(attention_output.size(0), attention_output.size(1)*attention_output.size(2))\n",
    "        print(attention_output_as_2d.shape)\n",
    "        output = self.linear1(attention_output_as_2d)\n",
    "        print(output.shape)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear2(output)\n",
    "        print(output.shape)\n",
    "        output = self.relu(output)\n",
    "        output = self.linear3(output)\n",
    "        print(output.shape)\n",
    "        output = self.relu(output)\n",
    "        print(output)\n",
    "        output = self.sigmoid(output)\n",
    "        print(output.shape)\n",
    "        print(output)\n",
    "        return output\n",
    "    \n",
    "\n",
    "sequence = torch.ones(1, 100, dtype=torch.long)\n",
    "mods = torch.zeros(1, 100, dtype=torch.long)\n",
    "data = torch.cat((sequence, mods), 0).unsqueeze(0)\n",
    "model = TunableAttentionRegression()\n",
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RTPy3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
