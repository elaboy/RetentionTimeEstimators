{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "from src import tokenize\n",
    "from CustomDatasets.PeptidesWithRetentionTimes import PeptidesWithRetentionTimes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"CalibratorTestingMultipleFilesSmallFiltered.csv\")\n",
    "fullSequence_retentionTimeAvg_df = df[['FullSequence', 'Mean']]\n",
    "\n",
    "train_df, test_df = train_test_split(fullSequence_retentionTimeAvg_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42, shuffle=True)\n",
    "train_df.to_csv(r\"trypsin_train_set.csv\", index=False)\n",
    "test_df.to_csv(r\"trypsin_test_set.csv\", index=False)\n",
    "val_df.to_csv(r\"trypsin_val_set.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                             FullSequence        Mean\n",
       " 196821                                         GGQVYATNTR   23.613440\n",
       " 6096                                    SQVFSTAADGQTQVEIK   68.222373\n",
       " 98887                                           QFAALVASK   47.242717\n",
       " 197496              GLAGGAALLSE[Metal:Cu[I] on E]WKGTGPGK  117.777690\n",
       " 94287   VIHDN[Common Artifact:Ammonia loss on N]FGIVE[...  164.517150\n",
       " ...                                                   ...         ...\n",
       " 119879                                   RQVLLSAAEAAEVILR  177.456603\n",
       " 103694  WN[Common Biological:Hydroxylation on N]GHTDM[...   74.850930\n",
       " 131932       VSGAQEM[Common Variable:Oxidation on M]VSSAK   17.015732\n",
       " 146867                                            THVGSYK   11.407020\n",
       " 121958                                      WVEQHLGPQFVER   81.213750\n",
       " \n",
       " [168844 rows x 2 columns],\n",
       "                                         FullSequence        Mean\n",
       " 118828                             IEATYESVLFHQLQEIK  142.002850\n",
       " 165033                                  TRLGLDDFESLK  103.061350\n",
       " 171469                                     LLDWLVDRR  123.112960\n",
       " 307                   SGAHSFISGLPQGYDTEVDEAGSQLSGGQR  114.029116\n",
       " 192399                          LAPSFPSPPAVSIASFVTVK  183.976660\n",
       " ...                                              ...         ...\n",
       " 2331                            SEKGVIQVYDLGQDGQGMSR   79.477970\n",
       " 97121                      QNLDELGIGTYHNIAFIHPDTPIIK  158.090590\n",
       " 163472  M[Common Variable:Oxidation on M]IEEAGAIISTR   68.688480\n",
       " 190275                                     LEMWLQNPK   99.386745\n",
       " 128805                                ERPSSAIYPSDSFR   46.231908\n",
       " \n",
       " [21106 rows x 2 columns],\n",
       "                                              FullSequence        Mean\n",
       " 153492  LC[Common Fixed:Carbamidomethyl on C]GSGFQ[Com...  120.584910\n",
       " 38285                                         GVVQLFNAVQK   96.679576\n",
       " 76233                                           EMYGAEWPK   73.667343\n",
       " 154829  LAATEQYHQVLC[Common Fixed:Carbamidomethyl on C...   89.266960\n",
       " 60685   [Common Biological:Acetylation on X]M[Common V...   18.821270\n",
       " ...                                                   ...         ...\n",
       " 137945      FELALQ[Common Artifact:Deamidation on Q]LGELK  143.787690\n",
       " 45840                                TLKPGTMSPKAFLEEAQVMK  151.046495\n",
       " 26322                                          HHQYEQHWYK   40.234159\n",
       " 190781                                        SFSTLFLETVK  119.008890\n",
       " 170229                                         IAEVGAGGNK   18.802580\n",
       " \n",
       " [21106 rows x 2 columns])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TunableAttentionRegression(torch.nn.Module):\n",
    "    def __init__(self, input_size = 2707, hidden_size = 128,\n",
    "                output_size = 1, numberOfHeads = 16) -> None:\n",
    "        super(TunableAttentionRegression, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_size, 32)\n",
    "        self.lstm = torch.nn.LSTM(32, hidden_size, batch_first=True)\n",
    "        self.attention = torch.nn.MultiheadAttention(hidden_size, num_heads=numberOfHeads, batch_first=True) #https://pytorch.org/docs/stable/generated/torch.ao.nn.quantizable.MultiheadAttention.html#multiheadattention\n",
    "        self.linear1 = torch.nn.Linear(hidden_size*200, output_size, dtype=torch.float32)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), x.size(2)*2)\n",
    "        # print(x.shape)\n",
    "        embedded = self.embedding(x)\n",
    "        # print(embedded.shape)\n",
    "        # embedded = embedded.view(*x.size(), -1) #to make it 3D\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        # print(lstm_out.shape)\n",
    "        # lstm_out = lstm_out.permute(1, 0, 2)  # [seq_len, batch, hidden_size]\n",
    "        # print(lstm_out.shape)\n",
    "        query = lstm_out.permute(0, 1, 2)\n",
    "        key = lstm_out.permute(0, 1, 2)\n",
    "        value = lstm_out.permute(0, 1, 2)\n",
    "        # print(query.shape, key.shape, value.shape)\n",
    "        attention_output, _ = self.attention(query, key, value)\n",
    "        # print(attention_output.shape)\n",
    "        attention_output_as_2d = attention_output.reshape(attention_output.size(0), attention_output.size(1)*attention_output.size(2))\n",
    "        # print(attention_output_as_2d.shape)\n",
    "        output = self.linear1(attention_output_as_2d)\n",
    "        # print(output.shape)\n",
    "        # print(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elabo\\Documents\\GitHub\\RetentionTimeEstimators\\src\\tokenize.py:29: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  sequence = str(row[0])\n",
      "C:\\Users\\elabo\\Documents\\GitHub\\RetentionTimeEstimators\\src\\tokenize.py:37: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preTokens.append((sequence, row[1]))\n",
      "C:\\Users\\elabo\\Documents\\GitHub\\RetentionTimeEstimators\\src\\tokenize.py:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  preTokens.append((removedColon, row[1]))\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenize.readVocabulary(\"vocab.csv\")\n",
    "training_data = pd.read_csv(r\"trypsin_train_set.csv\", index_col=None, header=0, usecols=[\"FullSequence\", \"Mean\"])\n",
    "testing_data = pd.read_csv(r\"trypsin_test_set.csv\", index_col=None, header=0, usecols=[\"FullSequence\", \"Mean\"])\n",
    "validation_data = pd.read_csv(r\"trypsin_val_set.csv\", index_col=None, header=0, usecols=[\"FullSequence\", \"Mean\"])\n",
    "\n",
    "trainingTokens = tokenize.tokenizePreTokens(tokenize.getPreTokens(training_data), vocab, 100, tokenize.TokenFormat.TwoDimensional)\n",
    "testingTokens = tokenize.tokenizePreTokens(tokenize.getPreTokens(testing_data), vocab, 100, tokenize.TokenFormat.TwoDimensional)\n",
    "validationTokens = tokenize.tokenizePreTokens(tokenize.getPreTokens(validation_data), vocab, 100, tokenize.TokenFormat.TwoDimensional)\n",
    "\n",
    "trainingSequences = []\n",
    "trainingRetentionTimes = []\n",
    "for i in trainingTokens:\n",
    "    trainingSequences.append(i[0])\n",
    "    trainingRetentionTimes.append(i[1])\n",
    "\n",
    "validationSequences = []\n",
    "validationRetentionTimes = []\n",
    "for i in validationTokens:\n",
    "    validationSequences.append(i[0])\n",
    "    validationRetentionTimes.append(i[1])\n",
    "\n",
    "testingSequences = []\n",
    "testingRetentionTimes = []\n",
    "for i in testingTokens:\n",
    "    testingSequences.append(i[0])\n",
    "    testingRetentionTimes.append(i[1])\n",
    "\n",
    "trainingDataset = PeptidesWithRetentionTimes(trainingSequences, trainingRetentionTimes)\n",
    "testingDataset = PeptidesWithRetentionTimes(testingSequences, testingRetentionTimes)\n",
    "validationDataset = PeptidesWithRetentionTimes(validationSequences, validationRetentionTimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131641, 16276, 16423)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainingDataset), len(testingDataset), len(validationDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data/ validation data\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create the model\n",
    "model = TunableAttentionRegression(hidden_size=128, numberOfHeads=16).to(device)\n",
    "# Create the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5)\n",
    "\n",
    "# Create the loss function\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "# Create the training data loader\n",
    "trainingDataLoader = torch.utils.data.DataLoader(trainingDataset,\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True, drop_last=True)\n",
    "\n",
    "validationDataLoader = torch.utils.data.DataLoader(validationDataset,\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True, drop_last=True)\n",
    "\n",
    "testinDataLoader = torch.utils.data.DataLoader(testingDataset,\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs\u001b[38;5;241m.\u001b[39mto(device))\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# print(outputs, labels.shape)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, \u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.train()\n",
    "epochs = 25\n",
    "validationLoss = []\n",
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_steps = 0\n",
    "    for i, data in enumerate(trainingDataLoader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = labels.float()\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # print(inputs[:,0].shape, labels.shape)\n",
    "        outputs = model(inputs.to(device)).to(device).squeeze()\n",
    "        # print(outputs, labels.shape)\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        epoch_steps += 1\n",
    "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
    "            print(\"[%d, %5d] loss: %.5f\" % (epoch + 1, i + 1,\n",
    "                                            running_loss / epoch_steps))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Validation loss\n",
    "    val_loss = 0.0\n",
    "    val_steps = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i, data in enumerate(validationDataLoader):\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = data\n",
    "            labels = labels.float()\n",
    "\n",
    "            outputs = model(inputs.to(device)).to(device).squeeze()\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs.to(device) == labels.to(device)).sum().item()\n",
    "\n",
    "            loss = criterion(outputs.to(device), labels.to(device))\n",
    "            val_loss += loss.cpu().numpy()\n",
    "            val_steps += 1\n",
    "    validationLoss.append(val_loss)\n",
    "    scheduler.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {val_loss:.5f}')\n",
    "\n",
    "model.eval()\n",
    "#save the model \n",
    "# torch.save(model.state_dict(), \"trypsin_lsm_attention_{}.pt\".format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the loss\n",
    "plt.plot(validationLoss)\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "#trendline\n",
    "z = np.polyfit(range(0, epochs), validationLoss, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(range(0, epochs), p(range(0, epochs)), \"r--\")\n",
    "plt.legend([\"Loss\", \"Trendline\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test model and get accuracy\n",
    "testinDataLoader = torch.utils.data.DataLoader(testingDataset,\n",
    "                                                batch_size=1024,\n",
    "                                                shuffle=True, drop_last=True)\n",
    "test_loss = 0.0\n",
    "test_steps = 0\n",
    "total = 0\n",
    "correct = 0\n",
    "# model.load_state_dict(torch.load('secondTry_model_hs_32_epoch_5.pt'))\n",
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "# Test model and calculate accuracy\n",
    "for i, data in enumerate(testinDataLoader):\n",
    "    with torch.no_grad():\n",
    "        inputs, labels = data\n",
    "\n",
    "        outputs = model(inputs.to(device)).to(device)\n",
    "        predicted = outputs\n",
    "        preds.append((labels, predicted))\n",
    "\n",
    "allPredictions = []\n",
    "allLabels = []\n",
    "for label, pred in preds:\n",
    "    allPredictions.append(pred)\n",
    "    allLabels.append(label)\n",
    "\n",
    "flatPreds = torch.stack(allPredictions).tolist()\n",
    "flatLabels = torch.stack(allLabels).tolist()\n",
    "\n",
    "# Plot the results\n",
    "#title\n",
    "plt.title(\"Trypsin Model\")\n",
    "#scatter plot\n",
    "plt.scatter(flatLabels, flatPreds, s=0.2)\n",
    "plt.plot(flatLabels, flatLabels, color='red')\n",
    "#labels\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "#increase plot size \n",
    "plt.gcf().set_size_inches(12, 8)\n",
    "#show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
